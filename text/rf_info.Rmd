The **random forest** approach is an ensemble-based learning method for classification. Random forest involves training models with a technique called bootstrap aggregation, a method of re-sampling the data with replacement and fitting a decision tree many times. The objective behind bootstrap aggregation is to increase the performance of a model by decreasing the variance of the model's predictions. 

Furthermore, random forest employs an algorithm which alters the learning process of the fitted trees. Each decision tree is fit with only a random subset of predictors, $m$. This prevents one variable from dominating all tree fits. It also prevents trees being correlated with one another. An optimal level of $m$ can be determined through cross validation. Random Forest computes the majority vote, or the class selected by most trees, to make a final prediction.

One disadvantage of random forest models over logistic regression models and single fitted trees, is their lack of interpretability. It becomes more difficult to examine the relationship between the response variable and the predictor variables when estimates are obtained by majority vote. For this reason, a variable importance plot is included alongside the random forest model output in the model fitting section.